{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyN3WliQmDwuG3aS8Fo+7Gx7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uYMKAnlJg76B","executionInfo":{"status":"ok","timestamp":1733124912506,"user_tz":-480,"elapsed":2462758,"user":{"displayName":"morris wu","userId":"06324333864864669834"}},"outputId":"a97788d4-1c1a-49fc-b469-8bb92f9d37ee"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","<ipython-input-7-14724b9c5caf>:119: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = amp.GradScaler()  # Initialize GradScaler for mixed precision\n","Epoch 1/1:   0%|          | 0/2300 [00:00<?, ?it/s]<ipython-input-7-14724b9c5caf>:131: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with amp.autocast():  # Enable mixed precision\n","Epoch 1/1: 100%|██████████| 2300/2300 [40:26<00:00,  1.05s/it, loss=0.728]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 1.0648\n"]},{"output_type":"stream","name":"stderr","text":["Prediction:   0%|          | 0/1 [00:00<?, ?it/s]<ipython-input-7-14724b9c5caf>:160: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with amp.autocast():  # Enable mixed precision during inference\n","Prediction: 100%|██████████| 1/1 [00:00<00:00, 21.88it/s]"]},{"output_type":"stream","name":"stdout","text":["Submission saved successfully!\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# Import Libraries\n","import os\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, AutoModel, AutoConfig\n","from transformers import AdamW, get_scheduler\n","import torch.nn as nn\n","\n","# Configuration\n","class CFG:\n","    seed = 42\n","    model_name = \"microsoft/deberta-v3-small\"\n","    max_length = 512\n","    epochs = 1\n","    batch_size = 20\n","    lr = 2e-5\n","    label2name = {0: 'winner_model_a', 1: 'winner_model_b', 2: 'winner_tie'}\n","    name2label = {v: k for k, v in label2name.items()}\n","\n","# Device Configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Reproducibility\n","torch.manual_seed(CFG.seed)\n","np.random.seed(CFG.seed)\n","\n","# Dataset Class\n","class PromptResponseDataset(Dataset):\n","    def __init__(self, texts, labels=None, tokenizer=None, max_length=CFG.max_length):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        inputs = self.tokenizer(self.texts[idx], truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\")\n","        if self.labels is not None:\n","            label = torch.tensor(self.labels[idx], dtype=torch.long)\n","            return {**inputs, \"labels\": label}\n","        return inputs\n","\n","# Load Data\n","df = pd.read_csv('./train.csv')\n","test_df = pd.read_csv('./test.csv')\n","\n","# Preprocessing\n","def make_pairs(row):\n","    try: prompt = row.prompt.encode(\"utf-8\").decode(\"utf-8\")\n","    except: prompt = \"\"\n","    try: response_a = row.response_a.encode(\"utf-8\").decode(\"utf-8\")\n","    except: response_a = \"\"\n","    try: response_b = row.response_b.encode(\"utf-8\").decode(\"utf-8\")\n","    except: response_b = \"\"\n","    row['options'] = [f\"Prompt: {prompt}\\n\\nResponse: {response_a}\", f\"Prompt: {prompt}\\n\\nResponse: {response_b}\"]\n","    return row\n","\n","df = df.apply(make_pairs, axis=1)\n","test_df = test_df.apply(make_pairs, axis=1)\n","df[\"class_label\"] = df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].idxmax(axis=1).map(CFG.name2label)\n","\n","# Split Data\n","train_df, valid_df = train_test_split(df, test_size=0.2, stratify=df[\"class_label\"])\n","\n","# Tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True, add_prefix_space=False)\n","\n","# Datasets and Dataloaders\n","train_dataset = PromptResponseDataset(train_df[\"options\"].tolist(), train_df[\"class_label\"].tolist(), tokenizer)\n","valid_dataset = PromptResponseDataset(valid_df[\"options\"].tolist(), valid_df[\"class_label\"].tolist(), tokenizer)\n","test_dataset = PromptResponseDataset(test_df[\"options\"].tolist(), tokenizer=tokenizer)\n","\n","train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True)\n","valid_loader = DataLoader(valid_dataset, batch_size=CFG.batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False)\n","\n","# Model\n","class DebertaV3Classifier(nn.Module):\n","    def __init__(self, model_name, num_classes=3):\n","        super().__init__()\n","        self.backbone = AutoModel.from_pretrained(model_name)\n","        self.dropout = nn.Dropout(0.2)\n","        self.classifier = nn.Linear(self.backbone.config.hidden_size * 2, num_classes)\n","\n","    def forward(self, input_ids, attention_mask):\n","        # Get outputs for both response A and response B\n","        outputs_a = self.backbone(input_ids[:, 0], attention_mask=attention_mask[:, 0])\n","        outputs_b = self.backbone(input_ids[:, 1], attention_mask=attention_mask[:, 1])\n","\n","        # Pool embeddings (mean-pooling across sequence length)\n","        pooled_output_a = torch.mean(outputs_a.last_hidden_state, dim=1)\n","        pooled_output_b = torch.mean(outputs_b.last_hidden_state, dim=1)\n","\n","        # Concatenate the embeddings\n","        pooled_output = torch.cat((pooled_output_a, pooled_output_b), dim=-1)\n","        pooled_output = self.dropout(pooled_output)\n","\n","        # Classification\n","        logits = self.classifier(pooled_output)\n","        return logits\n","\n","model = DebertaV3Classifier(CFG.model_name).to(device)\n","\n","# Loss, Optimizer, Scheduler\n","criterion = nn.CrossEntropyLoss()\n","optimizer = AdamW(model.parameters(), lr=CFG.lr)\n","scheduler = get_scheduler(\"linear\", optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * CFG.epochs)\n","\n","from tqdm import tqdm\n","import torch.cuda.amp as amp  # Mixed precision\n","\n","# Training Loop with TQDM and Mixed Precision\n","scaler = amp.GradScaler()  # Initialize GradScaler for mixed precision\n","\n","for epoch in range(CFG.epochs):\n","    model.train()\n","    train_loss = 0\n","    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{CFG.epochs}\", leave=True)  # TQDM progress bar\n","\n","    for batch in loop:\n","        inputs = {k: v.squeeze(1).to(device) for k, v in batch.items() if k != \"labels\" and k != \"token_type_ids\"}\n","        labels = batch[\"labels\"].to(device)\n","\n","        optimizer.zero_grad()\n","        with amp.autocast():  # Enable mixed precision\n","            outputs = model(**inputs)\n","            loss = criterion(outputs, labels)\n","\n","        # Backpropagation with mixed precision scaling\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        train_loss += loss.item()\n","\n","        # Update TQDM progress bar\n","        loop.set_postfix(loss=loss.item())\n","\n","    print(f\"Epoch {epoch+1}, Train Loss: {train_loss / len(train_loader):.4f}\")\n","\n","# Save Model\n","torch.save(model.state_dict(), \"deberta_model.pth\")\n","\n","# Prediction Loop with TQDM\n","model.eval()\n","predictions = []\n","ids = test_df[\"id\"].tolist()  # 提取 id 列\n","probs = []  # 用於存儲每行的概率分布\n","\n","with torch.no_grad():\n","    loop = tqdm(test_loader, desc=\"Prediction\", leave=True)  # TQDM progress bar\n","    for batch in loop:\n","        inputs = {k: v.squeeze(1).to(device) for k, v in batch.items() if k != \"token_type_ids\"}\n","        with amp.autocast():  # Enable mixed precision during inference\n","            outputs = model(**inputs)\n","        # 使用 softmax 將 logits 轉換為概率\n","        batch_probs = torch.softmax(outputs, dim=-1).cpu().tolist()\n","        probs.extend(batch_probs)\n","\n","# 保存到 DataFrame\n","submission = pd.DataFrame({\n","    \"id\": ids,\n","    \"winner_model_a\": [p[0] for p in probs],\n","    \"winner_model_b\": [p[1] for p in probs],\n","    \"winner_tie\": [p[2] for p in probs],\n","})\n","\n","# 保存為 CSV\n","submission.to_csv(\"submission.csv\", index=False)\n","print(\"Submission saved successfully!\")\n","\n"]}]}