# -*- coding: utf-8 -*-
"""NLP_HW3_NYCU_ 111550177.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eVay10n5EETU9x8F0ZRWJrb4newJg5Dh
"""


"""
Usage with GAI or Internet sources : 
1.	OpenAI ChatGPT
2.	Google Gemini
3.	Hugging Face 
4.	Google Colab AI
"""


"""
程式執行環境 : 
-	Python version : 3.9.0
-	Hardware : 
    我在 local environment (我的電腦) 執行 HW3 的程式。
    CPU : AMD Ryzen 5 7500F 6-Core Processor
    GPU : NVIDIA GeForce RTX 4060 Ti 16GB
    記憶體 : 32GB
    磁碟 : 2TB
-	Python Packages : 
    請查看 requirements.txt。
"""


import transformers as T
from datasets import load_dataset
import torch
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from tqdm import tqdm
from torchmetrics import SpearmanCorrCoef, Accuracy, F1Score, PearsonCorrCoef
import numpy as np
import random
import os
device = "cuda:0" if torch.cuda.is_available() else "cpu"

# 我在這裡設定 random seed，確保訓練結果一致
def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)

# 有些中文的標點符號在tokenizer編碼以後會變成[UNK]，所以將其換成英文標點
token_replacement = [
    ["：" , ":"],
    ["，" , ","],
    ["“" , "\""],
    ["”" , "\""],
    ["？" , "?"],
    ["……" , "..."],
    ["！" , "!"]
]

# 這邊我把 model = MultiLabelModel().to(device) 註解掉，因為 model 在這裡還沒被定義，我把這行改到下面
#model = MultiLabelModel().to(device)
"""
這裡我選擇 sentence-transformers/bert-base-nli-mean-tokens 作為我的模型，因為我們這次作業的任務 entailment
就是 nli (natural language inference) 的問題，使用 fine tune 在 nli dataset 的模型我認為會取得更好
的結果。
我有設定 model_max_length=512 來限制最大 token 數量，但在我們這次作業的 SemEval 2014 Task 1 不會有 token
數量超過的情況。
"""
tokenizer = T.BertTokenizer.from_pretrained("sentence-transformers/bert-base-nli-mean-tokens", model_max_length=512, cache_dir="./cache/")

class SemevalDataset(Dataset):
    def __init__(self, split="train") -> None:
        super().__init__()
        assert split in ["train", "validation", "test"] # 我在這裡加上了 test 讓我能夠在完成 Todo 的 training 之後對 test set 做 evaluation
        self.data = load_dataset(
            "sem_eval_2014_task_1", split=split, cache_dir="./cache/", trust_remote_code=True
        ).to_list()

    def __getitem__(self, index):
        d = self.data[index]
        # 把中文標點替換掉
        for k in ["premise", "hypothesis"]:
            for tok in token_replacement:
                d[k] = d[k].replace(tok[0], tok[1])
        return d

    def __len__(self):
        return len(self.data)

data_sample = SemevalDataset(split="train").data[:3]
print(f"Dataset example: \n{data_sample[0]} \n{data_sample[1]} \n{data_sample[2]}")

# Define the hyperparameters
lr = 8e-6
epochs = 28
train_batch_size = 16
validation_batch_size = 8

# TODO1: Create batched data for DataLoader
# `collate_fn` is a function that defines how the data batch should be packed.
# This function will be called in the DataLoader to pack the data batch.

def collate_fn(batch):
    # TODO1-1: Implement the collate_fn function
    # Write your code here
    # The input parameter is a data batch (tuple), and this function packs it into tensors.
    # Use tokenizer to pack tokenize and pack the data and its corresponding labels.
    # Return the data batch and labels for each sub-task.
    batch_size = len(batch)
    premise = [batch[i]['premise'] for i in range(batch_size)]
    hypothesis = [batch[i]['hypothesis'] for i in range(batch_size)]
    relatedness_score = [batch[i]['relatedness_score'] for i in range(batch_size)]
    entailment_judgment = [batch[i]['entailment_judgment'] for i in range(batch_size)]

    encoded_batch = tokenizer(premise, hypothesis, padding=True, truncation=True, return_tensors="pt")
    encoded_batch['relatedness_score'] = torch.tensor(relatedness_score)
    encoded_batch['entailment_judgment'] = torch.tensor(entailment_judgment)

    return encoded_batch

# TODO1-2: Define your DataLoader
dl_train = DataLoader(SemevalDataset(split="train"), batch_size=train_batch_size, shuffle=True, collate_fn=collate_fn)  # Write your code here
dl_validation = DataLoader(SemevalDataset(split="validation"), batch_size=validation_batch_size, shuffle=False, collate_fn=collate_fn)  # Write your code here

# TODO2: Construct your model
class MultiLabelModel(torch.nn.Module):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # Write your code here
        # Define what modules you will use in the model
        self.bert = T.BertModel.from_pretrained("sentence-transformers/bert-base-nli-mean-tokens", cache_dir="./cache/")

        self.share_1 = torch.nn.Linear(768, 768)

        self.separate_fc = torch.nn.Linear(768, 1024)
        self.separate_fc_2 = torch.nn.Linear(768, 1024)
        self.separate_pass = torch.nn.Linear(1024, 1024)

        self.separate_fc_5 = torch.nn.Linear(1024, 512)
        self.separate_fc_6 = torch.nn.Linear(1024, 512)
        self.separate_pass_2 = torch.nn.Linear(512, 512)

        self.separate_fc_3 = torch.nn.Linear(512, 256)
        self.separate_fc_4 = torch.nn.Linear(512, 256)
        self.relatedness_fc = torch.nn.Linear(256, 1)
        self.entailment_fc = torch.nn.Linear(256, 3)

        self.relu = torch.nn.ReLU()
        
        self.layer_norm = torch.nn.LayerNorm(768)
        self.layer_norm_1 = torch.nn.LayerNorm(1024)
        self.layer_norm_2 = torch.nn.LayerNorm(512)
        self.layer_norm_3 = torch.nn.LayerNorm(256)

    def forward(self, **kwargs):
        # Write your code here
        # Forward pass
        x = self.bert(**kwargs).pooler_output

        x = self.share_1(x)
        x = self.relu(x)
        x = self.layer_norm(x)

        x1 = self.separate_fc(x)
        x1 = self.relu(x1)
        x1 = self.layer_norm_1(x1)

        x2 = self.separate_fc_2(x)
        x2 = self.relu(x2)
        x2 = self.layer_norm_1(x2)

        x1 = self.separate_pass(x1)
        x1 = self.relu(x1)
        x1 = self.layer_norm_1(x1)

        x2 = self.separate_pass(x2)
        x2 = self.relu(x2)
        x2 = self.layer_norm_1(x2)

        x1 = self.separate_fc_5(x1)
        x1 = self.relu(x1)
        x1 = self.layer_norm_2(x1)

        x2 = self.separate_fc_6(x2)
        x2 = self.relu(x2)
        x2 = self.layer_norm_2(x2)

        x1 = self.separate_pass_2(x1)
        x1 = self.relu(x1)
        x1 = self.layer_norm_2(x1)

        x2 = self.separate_pass_2(x2)
        x2 = self.relu(x2)
        x2 = self.layer_norm_2(x2)

        x1 = self.separate_fc_3(x1)
        x1 = self.relu(x1)
        x1 = self.layer_norm_3(x1)

        x2 = self.separate_fc_4(x2)
        x2 = self.relu(x2)
        x2 = self.layer_norm_3(x2)

        x1 = self.relatedness_fc(x1)
        x2 = self.entailment_fc(x2)

        x2 = torch.nn.functional.softmax(x2, dim=-1)
        return x1, x2

# TODO3: Define your optimizer and loss function

# TODO3-1: Define your Optimizer
# MultiLabelModel() 已經定義好了，所以在這邊加上 model = MultiLabelModel().to(device)
model = MultiLabelModel().to(device)
optimizer = AdamW(model.parameters(), lr=lr)  # Write your code here

# TODO3-2: Define your loss functions (you should have two)
# Write your code here
relatedness_score_loss = torch.nn.MSELoss()
entailment_judgment_loss = torch.nn.CrossEntropyLoss()

# scoring functions
# 我在這裡多加上 person correlation coefficient
pcc = PearsonCorrCoef()
spc = SpearmanCorrCoef()
acc = Accuracy(task="multiclass", num_classes=3)
f1 = F1Score(task="multiclass", num_classes=3, average='macro')

# 確保 saved_models folder 存在
if not os.path.exists(f"./saved_models"):
    os.makedirs(f"./saved_models")

for ep in range(epochs):
    pbar = tqdm(dl_train)
    pbar.set_description(f"Training epoch [{ep+1}/{epochs}]")
    model.train()
    # TODO4: Write the training loop
    # Write your code here
    # train your model
    # clear gradient
    # forward pass
    # compute loss
    # back-propagation
    # model optimization
    for batch in pbar:
        batch = {k: v.to(device) for k, v in batch.items()}
        optimizer.zero_grad()
        relatedness_score, entailment_judgment = model(**{'input_ids':batch['input_ids'], 
                                                          'token_type_ids':batch['token_type_ids'], 
                                                          'attention_mask':batch['attention_mask']})
        relatedness_score_loss_value = relatedness_score_loss(relatedness_score, batch['relatedness_score'].float())
        entailment_judgment_loss_value = entailment_judgment_loss(entailment_judgment, batch['entailment_judgment'])
        
        loss = 1.9*relatedness_score_loss_value + 0.1*entailment_judgment_loss_value
        loss.backward()
        optimizer.step()
        pbar.set_postfix({"loss": loss.item()})

    pbar = tqdm(dl_validation)
    pbar.set_description(f"Validation epoch [{ep+1}/{epochs}]")
    model.eval()
    # TODO5: Write the evaluation loop
    # Write your code here
    # Evaluate your model
    # Output all the evaluation scores (SpearmanCorrCoef, Accuracy, F1Score)

    all_relatedness_scores = []
    all_pred_relatedness_scores = []
    all_entailment_labels = []
    all_pred_entailment_labels = []

    with torch.no_grad():
        for batch in pbar:
            batch = {k: v.to(device) for k, v in batch.items()}
            relatedness_score, entailment_judgment = model(
                input_ids=batch['input_ids'],
                token_type_ids=batch['token_type_ids'],
                attention_mask=batch['attention_mask']
            )

            all_relatedness_scores.extend(batch['relatedness_score'].cpu().numpy())
            all_pred_relatedness_scores.extend(torch.squeeze(relatedness_score, dim=-1).cpu().numpy())
            all_entailment_labels.extend(batch['entailment_judgment'].cpu().numpy())
            all_pred_entailment_labels.extend(entailment_judgment.argmax(dim=1).cpu().numpy())

    all_relatedness_scores = torch.tensor(all_relatedness_scores, dtype=torch.float32)
    all_pred_relatedness_scores = torch.tensor(all_pred_relatedness_scores, dtype=torch.float32)
    all_entailment_labels = torch.tensor(all_entailment_labels, dtype=torch.long)
    all_pred_entailment_labels = torch.tensor(all_pred_entailment_labels, dtype=torch.long)

    overall_pearson_corr_coef = pcc(all_pred_relatedness_scores, all_relatedness_scores).item()
    overall_spearman_corr_coef = spc(all_pred_relatedness_scores, all_relatedness_scores).item()
    overall_accuracy = acc(all_pred_entailment_labels, all_entailment_labels).item()
    overall_f1_score = f1(all_pred_entailment_labels, all_entailment_labels).item() 

    # 我這裡除了還有 SpearmanCorrCoef, Accuracy, F1Score 之外，還有輸出 PersonCorrCoef     
    print(f"pearson_corr_coef: {overall_pearson_corr_coef:.4f}, spearman_corr_coef: {overall_spearman_corr_coef:.4f}, accuracy: {overall_accuracy:.4f}, f1_score: {overall_f1_score:.4f}")
    
    torch.save(model, f'./saved_models/ep{ep}.ckpt')

"""For test set predictions, you can write perform evaluation simlar to #TODO5."""
dl_test = DataLoader(SemevalDataset(split="test"), batch_size=1000, shuffle=False, collate_fn=collate_fn)  # Write your code here

for i in range(0, 28):

    print(f"Epoch {i+1}")
    model = torch.load(f'./saved_models/ep{i}.ckpt')

    overall_spearman_corr_coef = 0
    overall_accuracy = 0
    overall_f1_score = 0
    pbar = tqdm(dl_test)
    pbar.set_description(f"Testing")
    model.eval()
    model.to(device)

    all_relatedness_scores = []
    all_pred_relatedness_scores = []
    all_entailment_labels = []
    all_pred_entailment_labels = []

    with torch.no_grad():
        for batch in pbar:
            batch = {k: v.to(device) for k, v in batch.items()}

            relatedness_score, entailment_judgment = model(
                input_ids=batch['input_ids'],
                token_type_ids=batch['token_type_ids'],
                attention_mask=batch['attention_mask']
            )

            all_relatedness_scores.extend(batch['relatedness_score'].cpu().numpy())
            all_pred_relatedness_scores.extend(torch.squeeze(relatedness_score, dim=-1).cpu().numpy())
            all_entailment_labels.extend(batch['entailment_judgment'].cpu().numpy())
            all_pred_entailment_labels.extend(entailment_judgment.argmax(dim=1).cpu().numpy())

    all_relatedness_scores = torch.tensor(all_relatedness_scores, dtype=torch.float32)
    all_pred_relatedness_scores = torch.tensor(all_pred_relatedness_scores, dtype=torch.float32)
    all_entailment_labels = torch.tensor(all_entailment_labels, dtype=torch.long)
    all_pred_entailment_labels = torch.tensor(all_pred_entailment_labels, dtype=torch.long)

    overall_pearson_corr_coef = pcc(all_pred_relatedness_scores, all_relatedness_scores).item()
    overall_spearman_corr_coef = spc(all_pred_relatedness_scores, all_relatedness_scores).item()
    overall_accuracy = acc(all_pred_entailment_labels, all_entailment_labels).item()
    overall_f1_score = f1(all_pred_entailment_labels, all_entailment_labels).item()

    print(f"pearson_corr_coef: {overall_pearson_corr_coef:.4f}, spearman_corr_coef: {overall_spearman_corr_coef:.4f}, accuracy: {overall_accuracy:.4f}, f1_score: {overall_f1_score:.4f}")